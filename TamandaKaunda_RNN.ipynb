{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngabo-dev/sentiment-analysis_group3/blob/main/TamandaKaunda_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f1b86572",
      "metadata": {
        "id": "f1b86572",
        "outputId": "112fb686-146e-4997-cc4a-b8730c63b527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Data Cleaning & Extraction for Sentiment Analysis (Amazon Reviews)\n",
        "# - Uses content/Reviews.csv\n",
        "# - Produces TF-IDF features and tokenized sequences\n",
        "# - Saves train/val/test splits and artifacts in processed_data/\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy import sparse\n",
        "\n",
        "# NLTK setup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def _ensure_nltk():\n",
        "    try:\n",
        "        _ = stopwords.words(\"english\")\n",
        "    except LookupError:\n",
        "        nltk.download(\"stopwords\")\n",
        "    try:\n",
        "        _ = nltk.data.find(\"corpora/wordnet\")\n",
        "    except LookupError:\n",
        "        nltk.download(\"wordnet\")\n",
        "    try:\n",
        "        _ = nltk.data.find(\"tokenizers/punkt\")\n",
        "    except LookupError:\n",
        "        nltk.download(\"punkt\")\n",
        "\n",
        "_ensure_nltk()\n",
        "STOP_WORDS = set(stopwords.words(\"english\"))\n",
        "LEMM = WordNetLemmatizer()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d351fc2f",
      "metadata": {
        "id": "d351fc2f"
      },
      "source": [
        "\n",
        "1) Load dataset (Amazon Reviews)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "113f27e0",
      "metadata": {
        "id": "113f27e0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_dataset():\n",
        "    candidates = [\n",
        "        os.path.join(\"content\", \"Reviews.csv\"),\n",
        "        os.path.join(\"content\", \"dataset_part1_random_70pct.csv\"),\n",
        "    ]\n",
        "    for path in candidates:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Loading dataset: {path}\")\n",
        "            df = pd.read_csv(path)\n",
        "            return df, path\n",
        "    raise FileNotFoundError(\"No dataset found in 'content/'. Expected Reviews.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "abd2afbb",
      "metadata": {
        "id": "abd2afbb"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2) Normalize columns and labels\n",
        "# -----------------------------\n",
        "def normalize_and_label(df):\n",
        "    # Expecting columns: 'Text' and 'Score'\n",
        "    # Map Score 1-2 -> 0 (negative), 4-5 -> 1 (positive), drop 3 (neutral)\n",
        "    text_col = None\n",
        "    for c in [\"Text\", \"text\", \"reviewText\", \"ReviewText\", \"review\", \"Review\"]:\n",
        "        if c in df.columns:\n",
        "            text_col = c\n",
        "            break\n",
        "    if text_col is None:\n",
        "        raise ValueError(\"Could not find text column. Expected 'Text' in Reviews.csv\")\n",
        "\n",
        "    if \"Score\" not in df.columns:\n",
        "        # If alternative sentiment column exists, try mapping\n",
        "        if \"Sentiment\" in df.columns:\n",
        "            df = df.rename(columns={text_col: \"text\", \"Sentiment\": \"sentiment\"})\n",
        "            df[\"label\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
        "            df = df.dropna(subset=[\"text\", \"label\"])\n",
        "            return df[[\"text\", \"label\"]]\n",
        "        else:\n",
        "            raise ValueError(\"Expected 'Score' column for Amazon Reviews dataset\")\n",
        "\n",
        "    df = df.rename(columns={text_col: \"text\"})\n",
        "    df = df[[\"text\", \"Score\"]].dropna(subset=[\"text\", \"Score\"])\n",
        "    # Drop neutral 3\n",
        "    df = df[df[\"Score\"] != 3]\n",
        "    df[\"label\"] = (df[\"Score\"] >= 4).astype(int)\n",
        "    df = df.drop(columns=[\"Score\"]).reset_index(drop=True)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "94b71664",
      "metadata": {
        "id": "94b71664"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 3) Text cleaning\n",
        "# -----------------------------\n",
        "def clean_text(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"<[^>]+>\", \" \", s)            # HTML tags\n",
        "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)   # URLs\n",
        "    s = re.sub(r\"[^a-z\\s]\", \" \", s)           # keep letters and space\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def lemmatize_and_filter(s: str) -> str:\n",
        "    tokens = s.split()  # simple fast tokenization on whitespace\n",
        "    kept = []\n",
        "    for t in tokens:\n",
        "        if t in STOP_WORDS:\n",
        "            continue\n",
        "        if len(t) < 3:\n",
        "            continue\n",
        "        kept.append(LEMM.lemmatize(t))\n",
        "    return \" \".join(kept)\n",
        "\n",
        "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    print(\"Cleaning text...\")\n",
        "    df = df.copy()\n",
        "    df[\"clean\"] = df[\"text\"].apply(clean_text)\n",
        "    print(\"Lemmatizing and removing stopwords...\")\n",
        "    df[\"proc\"] = df[\"clean\"].apply(lemmatize_and_filter)\n",
        "    # drop empty after processing\n",
        "    df = df[df[\"proc\"].str.len() > 0].reset_index(drop=True)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4ee8c917",
      "metadata": {
        "id": "4ee8c917"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 4) Split into train/val/test\n",
        "# -----------------------------\n",
        "def stratified_splits(df, test_size=0.2, val_size=0.1, seed=42):\n",
        "    y = df[\"label\"].values\n",
        "    X = df[\"proc\"].values\n",
        "\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, stratify=y, random_state=seed\n",
        "    )\n",
        "    val_ratio = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_ratio, stratify=y_temp, random_state=seed\n",
        "    )\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "40c4a2fa",
      "metadata": {
        "id": "40c4a2fa"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 5) TF-IDF features (for ML models)\n",
        "# -----------------------------\n",
        "def build_tfidf(X_train, X_val, X_test, max_features=30000):\n",
        "    vect = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        ngram_range=(1,2),\n",
        "        min_df=2,\n",
        "        max_df=0.98\n",
        "    )\n",
        "    Xtr = vect.fit_transform(X_train)\n",
        "    Xv = vect.transform(X_val)\n",
        "    Xte = vect.transform(X_test)\n",
        "    return Xtr, Xv, Xte, vect\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Simple tokenizer + sequences (for DL models)\n",
        "# -----------------------------\n",
        "def build_vocab(texts, max_words=50000, min_freq=2):\n",
        "    from collections import Counter\n",
        "    cnt = Counter()\n",
        "    for s in texts:\n",
        "        cnt.update(s.split())\n",
        "    # keep tokens by freq\n",
        "    vocab = [w for w, f in cnt.items() if f >= min_freq]\n",
        "    vocab.sort(key=lambda w: (-cnt[w], w))\n",
        "    vocab = vocab[:max_words]\n",
        "    word_index = {w: i+1 for i, w in enumerate(vocab)}  # 0 reserved for PAD\n",
        "    return word_index, cnt\n",
        "\n",
        "def texts_to_padded_sequences(texts, word_index, max_len=200):\n",
        "    seqs = []\n",
        "    for s in texts:\n",
        "        ids = [word_index.get(w, 0) for w in s.split()]\n",
        "        if len(ids) >= max_len:\n",
        "            ids = ids[:max_len]\n",
        "        else:\n",
        "            ids = ids + [0] * (max_len - len(ids))\n",
        "        seqs.append(ids)\n",
        "    return np.array(seqs, dtype=np.int32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2ceca4c4",
      "metadata": {
        "id": "2ceca4c4",
        "outputId": "3c7ff4df-8b1c-4a42-d13e-dbac2e15db11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset: content/Reviews.csv\n",
            "Cleaning text...\n",
            "Lemmatizing and removing stopwords...\n",
            "Saved cleaned dataset -> processed_data/processed_dataset.csv\n",
            "Splits -> train: 49842, val: 7121, test: 14241\n",
            "Saved TF-IDF features and vectorizer.\n",
            "Saved tokenized padded sequences.\n",
            "Saved labels.\n",
            "GloVe not found. Using random embeddings.\n",
            "Saved embedding matrix.\n",
            "\n",
            "Loaded artifacts:\n",
            "- TF-IDF shapes: (49842, 30000) (7121, 30000) (14241, 30000)\n",
            "- Seq shapes: (49842, 200) (7121, 200) (14241, 200)\n",
            "- Embedding: (20530, 100)\n",
            "- y sizes: (49842,) (7121,) (14241,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, json, numpy as np\n",
        "from scipy import sparse\n",
        "import pickle\n",
        "\n",
        "os.makedirs(\"processed_data\", exist_ok=True)\n",
        "\n",
        "# 1) Load + normalize\n",
        "df_raw, used_path = load_dataset()\n",
        "df = normalize_and_label(df_raw)\n",
        "df = preprocess_dataframe(df)\n",
        "\n",
        "# Persist cleaned dataset\n",
        "cleaned_path = os.path.join(\"processed_data\", \"processed_dataset.csv\")\n",
        "df[[\"text\", \"proc\", \"label\"]].to_csv(cleaned_path, index=False, encoding=\"utf-8\")\n",
        "print(f\"Saved cleaned dataset -> {cleaned_path}\")\n",
        "\n",
        "# 2) Splits\n",
        "(X_train, y_train), (X_val, y_val), (X_test, y_test) = stratified_splits(\n",
        "    df, test_size=0.2, val_size=0.1, seed=42\n",
        ")\n",
        "print(f\"Splits -> train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
        "\n",
        "# 3) TF-IDF (for Logistic Regression)\n",
        "Xtr_tfidf, Xv_tfidf, Xte_tfidf, tfidf_vect = build_tfidf(X_train, X_val, X_test, max_features=30000)\n",
        "sparse.save_npz(os.path.join(\"processed_data\", \"X_train_tfidf.npz\"), Xtr_tfidf)\n",
        "sparse.save_npz(os.path.join(\"processed_data\", \"X_val_tfidf.npz\"),   Xv_tfidf)\n",
        "sparse.save_npz(os.path.join(\"processed_data\", \"X_test_tfidf.npz\"),  Xte_tfidf)\n",
        "with open(os.path.join(\"processed_data\", \"tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(tfidf_vect, f)\n",
        "print(\"Saved TF-IDF features and vectorizer.\")\n",
        "\n",
        "# 4) Sequences (for RNN/LSTM)\n",
        "word_index, _ = build_vocab(X_train, max_words=50000, min_freq=2)\n",
        "with open(os.path.join(\"processed_data\", \"word_index.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(word_index, f)\n",
        "\n",
        "max_len = 200\n",
        "Xtr_seq = texts_to_padded_sequences(X_train, word_index, max_len=max_len)\n",
        "Xv_seq  = texts_to_padded_sequences(X_val,   word_index, max_len=max_len)\n",
        "Xte_seq = texts_to_padded_sequences(X_test,  word_index, max_len=max_len)\n",
        "np.save(os.path.join(\"processed_data\", \"X_train_seq.npy\"), Xtr_seq)\n",
        "np.save(os.path.join(\"processed_data\", \"X_val_seq.npy\"),   Xv_seq)\n",
        "np.save(os.path.join(\"processed_data\", \"X_test_seq.npy\"),  Xte_seq)\n",
        "print(\"Saved tokenized padded sequences.\")\n",
        "\n",
        "# 5) Labels\n",
        "np.save(os.path.join(\"processed_data\", \"y_train.npy\"), y_train)\n",
        "np.save(os.path.join(\"processed_data\", \"y_val.npy\"),   y_val)\n",
        "np.save(os.path.join(\"processed_data\", \"y_test.npy\"),  y_test)\n",
        "print(\"Saved labels.\")\n",
        "\n",
        "# 6) Embedding matrix (GloVe if available, else random)\n",
        "glove_path = os.path.join(\"content\", \"glove.6B.100d.txt\")\n",
        "embed_dim = 100\n",
        "vocab_size = (max(word_index.values()) + 1) if word_index else 1\n",
        "rng = np.random.default_rng(42)\n",
        "emb = rng.normal(scale=0.02, size=(vocab_size, embed_dim)).astype(np.float32)\n",
        "emb[0] = 0.0  # PAD row\n",
        "\n",
        "if os.path.exists(glove_path):\n",
        "    print(f\"Loading GloVe from {glove_path} ...\")\n",
        "    found = 0\n",
        "    with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            token, vec = parts[0], parts[1:]\n",
        "            if len(vec) != embed_dim:\n",
        "                continue\n",
        "            if token in word_index:\n",
        "                emb[word_index[token]] = np.asarray(vec, dtype=np.float32)\n",
        "                found += 1\n",
        "    print(f\"GloVe matches: {found}/{vocab_size-1}\")\n",
        "else:\n",
        "    print(\"GloVe not found. Using random embeddings.\")\n",
        "\n",
        "np.save(os.path.join(\"processed_data\", \"embedding_matrix_glove100.npy\"), emb)\n",
        "print(\"Saved embedding matrix.\")\n",
        "\n",
        "# --------- Minimal quick check ---------\n",
        "X_train_tfidf = sparse.load_npz(os.path.join(\"processed_data\", \"X_train_tfidf.npz\"))\n",
        "X_val_tfidf   = sparse.load_npz(os.path.join(\"processed_data\", \"X_val_tfidf.npz\"))\n",
        "X_test_tfidf  = sparse.load_npz(os.path.join(\"processed_data\", \"X_test_tfidf.npz\"))\n",
        "\n",
        "X_train_seq = np.load(os.path.join(\"processed_data\", \"X_train_seq.npy\"))\n",
        "X_val_seq   = np.load(os.path.join(\"processed_data\", \"X_val_seq.npy\"))\n",
        "X_test_seq  = np.load(os.path.join(\"processed_data\", \"X_test_seq.npy\"))\n",
        "emb         = np.load(os.path.join(\"processed_data\", \"embedding_matrix_glove100.npy\"))\n",
        "\n",
        "y_train = np.load(os.path.join(\"processed_data\", \"y_train.npy\"))\n",
        "y_val   = np.load(os.path.join(\"processed_data\", \"y_val.npy\"))\n",
        "y_test  = np.load(os.path.join(\"processed_data\", \"y_test.npy\"))\n",
        "\n",
        "print(\"\\nLoaded artifacts:\")\n",
        "print(\"- TF-IDF shapes:\", X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape)\n",
        "print(\"- Seq shapes:\",   X_train_seq.shape, X_val_seq.shape, X_test_seq.shape)\n",
        "print(\"- Embedding:\",    emb.shape)\n",
        "print(\"- y sizes:\",      y_train.shape, y_val.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Model"
      ],
      "metadata": {
        "id": "a0LUemj6D7LQ"
      },
      "id": "a0LUemj6D7LQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
        "\n",
        "# Load the data from the 'processed_data' directory\n",
        "data_dir = \"processed_data\"\n",
        "\n",
        "X_train_seq = np.load(os.path.join(data_dir, \"X_train_seq.npy\"))\n",
        "X_val_seq = np.load(os.path.join(data_dir, \"X_val_seq.npy\"))\n",
        "X_test_seq = np.load(os.path.join(data_dir, \"X_test_seq.npy\"))\n",
        "\n",
        "y_train = np.load(os.path.join(data_dir, \"y_train.npy\"))\n",
        "y_val = np.load(os.path.join(data_dir, \"y_val.npy\"))\n",
        "y_test = np.load(os.path.join(data_dir, \"y_test.npy\"))\n",
        "\n",
        "emb = np.load(os.path.join(data_dir, \"embedding_matrix_glove100.npy\"))\n",
        "\n",
        "print(\"Loaded sequence data shapes:\")\n",
        "print(f\"X_train_seq: {X_train_seq.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"X_val_seq:   {X_val_seq.shape}, y_val:   {y_val.shape}\")\n",
        "print(f\"X_test_seq:  {X_test_seq.shape}, y_test:  {y_test.shape}\")\n",
        "print(f\"Embedding matrix shape: {emb.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjDkUyr2D_FY",
        "outputId": "ab98074f-c186-4571-e563-47ca6b545494"
      },
      "id": "TjDkUyr2D_FY",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded sequence data shapes:\n",
            "X_train_seq: (49842, 200), y_train: (49842,)\n",
            "X_val_seq:   (7121, 200), y_val:   (7121,)\n",
            "X_test_seq:  (14241, 200), y_test:  (14241,)\n",
            "Embedding matrix shape: (20530, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data dimensions from loaded data\n",
        "vocab_size = emb.shape[0]\n",
        "embed_dim = emb.shape[1]\n",
        "max_len = X_train_seq.shape[1]\n",
        "\n",
        "# Define the SimpleRNN model\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embed_dim,\n",
        "        weights=[emb],\n",
        "        input_length=max_len,\n",
        "        trainable=False  #  pre-trained GloVe weights\n",
        "    ),\n",
        "    SimpleRNN(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification output layer\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "AL7622h3FSHj",
        "outputId": "ed02d964-563c-44ec-9b27-7ee4cfe0cdd4"
      },
      "id": "AL7622h3FSHj",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m2,053,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,053,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,053,000\u001b[0m (7.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,053,000</span> (7.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,053,000\u001b[0m (7.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,053,000</span> (7.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model using the loaded sequences and labels\n",
        "history = model.fit(\n",
        "    X_train_seq,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val_seq, y_val)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWpIZdenJJUl",
        "outputId": "0056e419-c760-4729-a257-ec9588222e10"
      },
      "id": "ZWpIZdenJJUl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m779/779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 60ms/step - accuracy: 0.8449 - loss: 0.4356 - val_accuracy: 0.8448 - val_loss: 0.4324\n",
            "Epoch 2/20\n",
            "\u001b[1m779/779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.8430 - loss: 0.4386 - val_accuracy: 0.8448 - val_loss: 0.4313\n",
            "Epoch 3/20\n",
            "\u001b[1m605/779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.8425 - loss: 0.4380 "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}