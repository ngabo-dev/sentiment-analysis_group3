{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b86572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning & Extraction for Sentiment Analysis (Amazon Reviews)\n",
    "# - Uses content/Reviews.csv\n",
    "# - Produces TF-IDF features and tokenized sequences\n",
    "# - Saves train/val/test splits and artifacts in processed_data/\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "# NLTK setup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def _ensure_nltk():\n",
    "    try:\n",
    "        _ = stopwords.words(\"english\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\")\n",
    "    try:\n",
    "        _ = nltk.data.find(\"corpora/wordnet\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"wordnet\")\n",
    "    try:\n",
    "        _ = nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "_ensure_nltk()\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "LEMM = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351fc2f",
   "metadata": {},
   "source": [
    "\n",
    "1) Load dataset (Amazon Reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113f27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset():\n",
    "    candidates = [\n",
    "        os.path.join(\"content\", \"Reviews.csv\"),\n",
    "        os.path.join(\"content\", \"dataset_part1_random_70pct.csv\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Loading dataset: {path}\")\n",
    "            df = pd.read_csv(path)\n",
    "            return df, path\n",
    "    raise FileNotFoundError(\"No dataset found in 'content/'. Expected Reviews.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd2afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Normalize columns and labels\n",
    "# -----------------------------\n",
    "def normalize_and_label(df):\n",
    "    # Expecting columns: 'Text' and 'Score'\n",
    "    # Map Score 1-2 -> 0 (negative), 4-5 -> 1 (positive), drop 3 (neutral)\n",
    "    text_col = None\n",
    "    for c in [\"Text\", \"text\", \"reviewText\", \"ReviewText\", \"review\", \"Review\"]:\n",
    "        if c in df.columns:\n",
    "            text_col = c\n",
    "            break\n",
    "    if text_col is None:\n",
    "        raise ValueError(\"Could not find text column. Expected 'Text' in Reviews.csv\")\n",
    "\n",
    "    if \"Score\" not in df.columns:\n",
    "        # If alternative sentiment column exists, try mapping\n",
    "        if \"Sentiment\" in df.columns:\n",
    "            df = df.rename(columns={text_col: \"text\", \"Sentiment\": \"sentiment\"})\n",
    "            df[\"label\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
    "            df = df.dropna(subset=[\"text\", \"label\"])\n",
    "            return df[[\"text\", \"label\"]]\n",
    "        else:\n",
    "            raise ValueError(\"Expected 'Score' column for Amazon Reviews dataset\")\n",
    "\n",
    "    df = df.rename(columns={text_col: \"text\"})\n",
    "    df = df[[\"text\", \"Score\"]].dropna(subset=[\"text\", \"Score\"])\n",
    "    # Drop neutral 3\n",
    "    df = df[df[\"Score\"] != 3]\n",
    "    df[\"label\"] = (df[\"Score\"] >= 4).astype(int)\n",
    "    df = df.drop(columns=[\"Score\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b71664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Text cleaning\n",
    "# -----------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)            # HTML tags\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)   # URLs\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)           # keep letters and space\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def lemmatize_and_filter(s: str) -> str:\n",
    "    tokens = s.split()  # simple fast tokenization on whitespace\n",
    "    kept = []\n",
    "    for t in tokens:\n",
    "        if t in STOP_WORDS: \n",
    "            continue\n",
    "        if len(t) < 3:\n",
    "            continue\n",
    "        kept.append(LEMM.lemmatize(t))\n",
    "    return \" \".join(kept)\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"Cleaning text...\")\n",
    "    df = df.copy()\n",
    "    df[\"clean\"] = df[\"text\"].apply(clean_text)\n",
    "    print(\"Lemmatizing and removing stopwords...\")\n",
    "    df[\"proc\"] = df[\"clean\"].apply(lemmatize_and_filter)\n",
    "    # drop empty after processing\n",
    "    df = df[df[\"proc\"].str.len() > 0].reset_index(drop=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee8c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Split into train/val/test\n",
    "# -----------------------------\n",
    "def stratified_splits(df, test_size=0.2, val_size=0.1, seed=42):\n",
    "    y = df[\"label\"].values\n",
    "    X = df[\"proc\"].values\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=seed\n",
    "    )\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, stratify=y_temp, random_state=seed\n",
    "    )\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c4a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) TF-IDF features (for ML models)\n",
    "# -----------------------------\n",
    "def build_tfidf(X_train, X_val, X_test, max_features=30000):\n",
    "    vect = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_df=0.98\n",
    "    )\n",
    "    Xtr = vect.fit_transform(X_train)\n",
    "    Xv = vect.transform(X_val)\n",
    "    Xte = vect.transform(X_test)\n",
    "    return Xtr, Xv, Xte, vect\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Simple tokenizer + sequences (for DL models)\n",
    "# -----------------------------\n",
    "def build_vocab(texts, max_words=50000, min_freq=2):\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for s in texts:\n",
    "        cnt.update(s.split())\n",
    "    # keep tokens by freq\n",
    "    vocab = [w for w, f in cnt.items() if f >= min_freq]\n",
    "    vocab.sort(key=lambda w: (-cnt[w], w))\n",
    "    vocab = vocab[:max_words]\n",
    "    word_index = {w: i+1 for i, w in enumerate(vocab)}  # 0 reserved for PAD\n",
    "    return word_index, cnt\n",
    "\n",
    "def texts_to_padded_sequences(texts, word_index, max_len=200):\n",
    "    seqs = []\n",
    "    for s in texts:\n",
    "        ids = [word_index.get(w, 0) for w in s.split()]\n",
    "        if len(ids) >= max_len:\n",
    "            ids = ids[:max_len]\n",
    "        else:\n",
    "            ids = ids + [0] * (max_len - len(ids))\n",
    "        seqs.append(ids)\n",
    "    return np.array(seqs, dtype=np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ceca4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: content\\Reviews.csv\n",
      "Rows in raw: 158506\n",
      "Rows after label filtering (drop score=3): 146514\n",
      "Cleaning text...\n",
      "Lemmatizing and removing stopwords...\n",
      "Saved cleaned dataset -> processed_data\\processed_dataset.csv\n",
      "Splits -> train: 102557, val: 14652, test: 29303\n",
      "Saved TF-IDF features and vectorizer.\n",
      "Saved tokenized padded sequences.\n",
      "Saved labels.\n",
      "GloVe not found. Using random embeddings. (Optional: place glove.6B.100d.txt in content/)\n",
      "Saved embedding matrix (GloVe/random).\n",
      "\n",
      "All done. Files written to ./processed_data\n",
      "Loaded artifacts:\n",
      "- TF-IDF shapes: (102557, 30000) (14652, 30000) (29303, 30000)\n",
      "- Seq shapes: (102557, 200) (14652, 200) (29303, 200)\n",
      "- Embedding: (28533, 100)\n",
      "- y sizes: (102557,) (14652,) (29303,)\n"
     ]
    }
   ],
   "source": [
    "# --------- RUN ---------\n",
    "# Tip: Set limit_rows for a faster first run (e.g., 100000). None uses all rows.\n",
    "run_pipeline(limit_rows=None, seed=42)\n",
    "\n",
    "# Minimal quick check (no extra helper loaders)\n",
    "from scipy import sparse\n",
    "import numpy as np, os, json\n",
    "\n",
    "# TF-IDF (for Logistic Regression)\n",
    "X_train_tfidf = sparse.load_npz(os.path.join(\"processed_data\", \"X_train_tfidf.npz\"))\n",
    "X_val_tfidf   = sparse.load_npz(os.path.join(\"processed_data\", \"X_val_tfidf.npz\"))\n",
    "X_test_tfidf  = sparse.load_npz(os.path.join(\"processed_data\", \"X_test_tfidf.npz\"))\n",
    "\n",
    "# Sequences + Embedding (for RNN/LSTM)\n",
    "X_train_seq = np.load(os.path.join(\"processed_data\", \"X_train_seq.npy\"))\n",
    "X_val_seq   = np.load(os.path.join(\"processed_data\", \"X_val_seq.npy\"))\n",
    "X_test_seq  = np.load(os.path.join(\"processed_data\", \"X_test_seq.npy\"))\n",
    "emb         = np.load(os.path.join(\"processed_data\", \"embedding_matrix_glove100.npy\"))\n",
    "\n",
    "# Labels\n",
    "y_train = np.load(os.path.join(\"processed_data\", \"y_train.npy\"))\n",
    "y_val   = np.load(os.path.join(\"processed_data\", \"y_val.npy\"))\n",
    "y_test  = np.load(os.path.join(\"processed_data\", \"y_test.npy\"))\n",
    "\n",
    "print(\"Loaded artifacts:\")\n",
    "print(\"- TF-IDF shapes:\", X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape)\n",
    "print(\"- Seq shapes:\",   X_train_seq.shape, X_val_seq.shape, X_test_seq.shape)\n",
    "print(\"- Embedding:\",    emb.shape)\n",
    "print(\"- y sizes:\",      y_train.shape, y_val.shape, y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
