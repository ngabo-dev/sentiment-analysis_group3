{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b86572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning & Extraction for Sentiment Analysis (Amazon Reviews)\n",
    "# - Uses content/Reviews.csv\n",
    "# - Produces TF-IDF features and tokenized sequences\n",
    "# - Saves train/val/test splits and artifacts in processed_data/\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "# NLTK setup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def _ensure_nltk():\n",
    "    try:\n",
    "        _ = stopwords.words(\"english\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\")\n",
    "    try:\n",
    "        _ = nltk.data.find(\"corpora/wordnet\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"wordnet\")\n",
    "    try:\n",
    "        _ = nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "_ensure_nltk()\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "LEMM = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e022a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d351fc2f",
   "metadata": {},
   "source": [
    "\n",
    "1) Load dataset (Amazon Reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113f27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset():\n",
    "    candidates = [\n",
    "        os.path.join(\"content\", \"Reviews.csv\"),\n",
    "        os.path.join(\"content\", \"dataset_part1_random_70pct.csv\"),\n",
    "    ]\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Loading dataset: {path}\")\n",
    "            df = pd.read_csv(path)\n",
    "            return df, path\n",
    "    raise FileNotFoundError(\"No dataset found in 'content/'. Expected Reviews.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd2afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Normalize columns and labels\n",
    "# -----------------------------\n",
    "def normalize_and_label(df):\n",
    "    # Expecting columns: 'Text' and 'Score'\n",
    "    # Map Score 1-2 -> 0 (negative), 4-5 -> 1 (positive), drop 3 (neutral)\n",
    "    text_col = None\n",
    "    for c in [\"Text\", \"text\", \"reviewText\", \"ReviewText\", \"review\", \"Review\"]:\n",
    "        if c in df.columns:\n",
    "            text_col = c\n",
    "            break\n",
    "    if text_col is None:\n",
    "        raise ValueError(\"Could not find text column. Expected 'Text' in Reviews.csv\")\n",
    "\n",
    "    if \"Score\" not in df.columns:\n",
    "        # If alternative sentiment column exists, try mapping\n",
    "        if \"Sentiment\" in df.columns:\n",
    "            df = df.rename(columns={text_col: \"text\", \"Sentiment\": \"sentiment\"})\n",
    "            df[\"label\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
    "            df = df.dropna(subset=[\"text\", \"label\"])\n",
    "            return df[[\"text\", \"label\"]]\n",
    "        else:\n",
    "            raise ValueError(\"Expected 'Score' column for Amazon Reviews dataset\")\n",
    "\n",
    "    df = df.rename(columns={text_col: \"text\"})\n",
    "    df = df[[\"text\", \"Score\"]].dropna(subset=[\"text\", \"Score\"])\n",
    "    # Drop neutral 3\n",
    "    df = df[df[\"Score\"] != 3]\n",
    "    df[\"label\"] = (df[\"Score\"] >= 4).astype(int)\n",
    "    df = df.drop(columns=[\"Score\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b71664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Text cleaning\n",
    "# -----------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)            # HTML tags\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)   # URLs\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)           # keep letters and space\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def lemmatize_and_filter(s: str) -> str:\n",
    "    tokens = s.split()  # simple fast tokenization on whitespace\n",
    "    kept = []\n",
    "    for t in tokens:\n",
    "        if t in STOP_WORDS: \n",
    "            continue\n",
    "        if len(t) < 3:\n",
    "            continue\n",
    "        kept.append(LEMM.lemmatize(t))\n",
    "    return \" \".join(kept)\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"Cleaning text...\")\n",
    "    df = df.copy()\n",
    "    df[\"clean\"] = df[\"text\"].apply(clean_text)\n",
    "    print(\"Lemmatizing and removing stopwords...\")\n",
    "    df[\"proc\"] = df[\"clean\"].apply(lemmatize_and_filter)\n",
    "    # drop empty after processing\n",
    "    df = df[df[\"proc\"].str.len() > 0].reset_index(drop=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee8c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Split into train/val/test\n",
    "# -----------------------------\n",
    "def stratified_splits(df, test_size=0.2, val_size=0.1, seed=42):\n",
    "    y = df[\"label\"].values\n",
    "    X = df[\"proc\"].values\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=seed\n",
    "    )\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, stratify=y_temp, random_state=seed\n",
    "    )\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c4a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) TF-IDF features (for ML models)\n",
    "# -----------------------------\n",
    "def build_tfidf(X_train, X_val, X_test, max_features=30000):\n",
    "    vect = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_df=0.98\n",
    "    )\n",
    "    Xtr = vect.fit_transform(X_train)\n",
    "    Xv = vect.transform(X_val)\n",
    "    Xte = vect.transform(X_test)\n",
    "    return Xtr, Xv, Xte, vect\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Simple tokenizer + sequences (for DL models)\n",
    "# -----------------------------\n",
    "def build_vocab(texts, max_words=50000, min_freq=2):\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for s in texts:\n",
    "        cnt.update(s.split())\n",
    "    # keep tokens by freq\n",
    "    vocab = [w for w, f in cnt.items() if f >= min_freq]\n",
    "    vocab.sort(key=lambda w: (-cnt[w], w))\n",
    "    vocab = vocab[:max_words]\n",
    "    word_index = {w: i+1 for i, w in enumerate(vocab)}  # 0 reserved for PAD\n",
    "    return word_index, cnt\n",
    "\n",
    "def texts_to_padded_sequences(texts, word_index, max_len=200):\n",
    "    seqs = []\n",
    "    for s in texts:\n",
    "        ids = [word_index.get(w, 0) for w in s.split()]\n",
    "        if len(ids) >= max_len:\n",
    "            ids = ids[:max_len]\n",
    "        else:\n",
    "            ids = ids + [0] * (max_len - len(ids))\n",
    "        seqs.append(ids)\n",
    "    return np.array(seqs, dtype=np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ceca4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: content\\Reviews.csv\n",
      "Cleaning text...\n",
      "Lemmatizing and removing stopwords...\n",
      "Saved cleaned dataset -> processed_data\\processed_dataset.csv\n",
      "Splits -> train: 49842, val: 7121, test: 14241\n",
      "Saved TF-IDF features and vectorizer.\n",
      "Saved tokenized padded sequences.\n",
      "Saved labels.\n",
      "GloVe not found. Using random embeddings.\n",
      "Saved embedding matrix.\n",
      "\n",
      "Loaded artifacts:\n",
      "- TF-IDF shapes: (49842, 30000) (7121, 30000) (14241, 30000)\n",
      "- Seq shapes: (49842, 200) (7121, 200) (14241, 200)\n",
      "- Embedding: (20530, 100)\n",
      "- y sizes: (49842,) (7121,) (14241,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, numpy as np\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "os.makedirs(\"processed_data\", exist_ok=True)\n",
    "\n",
    "# 1) Load + normalize\n",
    "df_raw, used_path = load_dataset()\n",
    "df = normalize_and_label(df_raw)\n",
    "df = preprocess_dataframe(df)\n",
    "\n",
    "# Persist cleaned dataset\n",
    "cleaned_path = os.path.join(\"processed_data\", \"processed_dataset.csv\")\n",
    "df[[\"text\", \"proc\", \"label\"]].to_csv(cleaned_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved cleaned dataset -> {cleaned_path}\")\n",
    "\n",
    "# 2) Splits\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = stratified_splits(\n",
    "    df, test_size=0.2, val_size=0.1, seed=42\n",
    ")\n",
    "print(f\"Splits -> train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "# 3) TF-IDF (for Logistic Regression)\n",
    "Xtr_tfidf, Xv_tfidf, Xte_tfidf, tfidf_vect = build_tfidf(X_train, X_val, X_test, max_features=30000)\n",
    "sparse.save_npz(os.path.join(\"processed_data\", \"X_train_tfidf.npz\"), Xtr_tfidf)\n",
    "sparse.save_npz(os.path.join(\"processed_data\", \"X_val_tfidf.npz\"),   Xv_tfidf)\n",
    "sparse.save_npz(os.path.join(\"processed_data\", \"X_test_tfidf.npz\"),  Xte_tfidf)\n",
    "with open(os.path.join(\"processed_data\", \"tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(tfidf_vect, f)\n",
    "print(\"Saved TF-IDF features and vectorizer.\")\n",
    "\n",
    "# 4) Sequences (for RNN/LSTM)\n",
    "word_index, _ = build_vocab(X_train, max_words=50000, min_freq=2)\n",
    "with open(os.path.join(\"processed_data\", \"word_index.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_index, f)\n",
    "\n",
    "max_len = 200\n",
    "Xtr_seq = texts_to_padded_sequences(X_train, word_index, max_len=max_len)\n",
    "Xv_seq  = texts_to_padded_sequences(X_val,   word_index, max_len=max_len)\n",
    "Xte_seq = texts_to_padded_sequences(X_test,  word_index, max_len=max_len)\n",
    "np.save(os.path.join(\"processed_data\", \"X_train_seq.npy\"), Xtr_seq)\n",
    "np.save(os.path.join(\"processed_data\", \"X_val_seq.npy\"),   Xv_seq)\n",
    "np.save(os.path.join(\"processed_data\", \"X_test_seq.npy\"),  Xte_seq)\n",
    "print(\"Saved tokenized padded sequences.\")\n",
    "\n",
    "# 5) Labels\n",
    "np.save(os.path.join(\"processed_data\", \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(\"processed_data\", \"y_val.npy\"),   y_val)\n",
    "np.save(os.path.join(\"processed_data\", \"y_test.npy\"),  y_test)\n",
    "print(\"Saved labels.\")\n",
    "\n",
    "# 6) Embedding matrix (GloVe if available, else random)\n",
    "glove_path = os.path.join(\"content\", \"glove.6B.100d.txt\")\n",
    "embed_dim = 100\n",
    "vocab_size = (max(word_index.values()) + 1) if word_index else 1\n",
    "rng = np.random.default_rng(42)\n",
    "emb = rng.normal(scale=0.02, size=(vocab_size, embed_dim)).astype(np.float32)\n",
    "emb[0] = 0.0  # PAD row\n",
    "\n",
    "if os.path.exists(glove_path):\n",
    "    print(f\"Loading GloVe from {glove_path} ...\")\n",
    "    found = 0\n",
    "    with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(\" \")\n",
    "            token, vec = parts[0], parts[1:]\n",
    "            if len(vec) != embed_dim: \n",
    "                continue\n",
    "            if token in word_index:\n",
    "                emb[word_index[token]] = np.asarray(vec, dtype=np.float32)\n",
    "                found += 1\n",
    "    print(f\"GloVe matches: {found}/{vocab_size-1}\")\n",
    "else:\n",
    "    print(\"GloVe not found. Using random embeddings.\")\n",
    "\n",
    "np.save(os.path.join(\"processed_data\", \"embedding_matrix_glove100.npy\"), emb)\n",
    "print(\"Saved embedding matrix.\")\n",
    "\n",
    "# --------- Minimal quick check ---------\n",
    "X_train_tfidf = sparse.load_npz(os.path.join(\"processed_data\", \"X_train_tfidf.npz\"))\n",
    "X_val_tfidf   = sparse.load_npz(os.path.join(\"processed_data\", \"X_val_tfidf.npz\"))\n",
    "X_test_tfidf  = sparse.load_npz(os.path.join(\"processed_data\", \"X_test_tfidf.npz\"))\n",
    "\n",
    "X_train_seq = np.load(os.path.join(\"processed_data\", \"X_train_seq.npy\"))\n",
    "X_val_seq   = np.load(os.path.join(\"processed_data\", \"X_val_seq.npy\"))\n",
    "X_test_seq  = np.load(os.path.join(\"processed_data\", \"X_test_seq.npy\"))\n",
    "emb         = np.load(os.path.join(\"processed_data\", \"embedding_matrix_glove100.npy\"))\n",
    "\n",
    "y_train = np.load(os.path.join(\"processed_data\", \"y_train.npy\"))\n",
    "y_val   = np.load(os.path.join(\"processed_data\", \"y_val.npy\"))\n",
    "y_test  = np.load(os.path.join(\"processed_data\", \"y_test.npy\"))\n",
    "\n",
    "print(\"\\nLoaded artifacts:\")\n",
    "print(\"- TF-IDF shapes:\", X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape)\n",
    "print(\"- Seq shapes:\",   X_train_seq.shape, X_val_seq.shape, X_test_seq.shape)\n",
    "print(\"- Embedding:\",    emb.shape)\n",
    "print(\"- y sizes:\",      y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1d30df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1s/step - accuracy: 0.8433 - loss: 0.4390 - val_accuracy: 0.8448 - val_loss: 0.4142\n",
      "Epoch 2/3\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 1s/step - accuracy: 0.8453 - loss: 0.4082 - val_accuracy: 0.8438 - val_loss: 0.4007\n",
      "Epoch 3/3\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 1s/step - accuracy: 0.8456 - loss: 0.3969 - val_accuracy: 0.8457 - val_loss: 0.3965\n",
      "Test accuracy: 0.8448, F1: 0.9149\n"
     ]
    }
   ],
   "source": [
    "# Minimal LSTM for sentiment classification using processed_data\n",
    "import os, numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "X_train = np.load(os.path.join(\"processed_data\", \"X_train_seq.npy\"))\n",
    "X_val   = np.load(os.path.join(\"processed_data\", \"X_val_seq.npy\"))\n",
    "X_test  = np.load(os.path.join(\"processed_data\", \"X_test_seq.npy\"))\n",
    "y_train = np.load(os.path.join(\"processed_data\", \"y_train.npy\"))\n",
    "y_val   = np.load(os.path.join(\"processed_data\", \"y_val.npy\"))\n",
    "y_test  = np.load(os.path.join(\"processed_data\", \"y_test.npy\"))\n",
    "emb     = np.load(os.path.join(\"processed_data\", \"embedding_matrix_glove100.npy\"))\n",
    "\n",
    "max_len = X_train.shape[1]\n",
    "vocab_size, embed_dim = emb.shape\n",
    "\n",
    "# Model\n",
    "inp = layers.Input(shape=(max_len,), dtype=\"int32\")\n",
    "embd = layers.Embedding(vocab_size, embed_dim, weights=[emb], trainable=False, mask_zero=True)(inp)\n",
    "x = layers.LSTM(128)(embd)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = models.Model(inp, out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=1, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "pred = (model.predict(X_test, batch_size=512, verbose=0).ravel() >= 0.5).astype(int)\n",
    "print(f\"Test accuracy: {acc:.4f}, F1: {f1_score(y_test, pred):.4f}\")\n",
    "\n",
    "# Save\n",
    "model.save(os.path.join(\"processed_data\", \"lstm_glove.keras\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
